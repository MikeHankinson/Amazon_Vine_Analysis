{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16_9_1_pySpark_ETL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXRXsUffm0aB3G4/SRiKZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeHankinson/Amazon_Vine_Analysis/blob/main/16_9_1_pySpark_ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUT75igpE3yc"
      },
      "source": [
        "**Install PySpark** \r\n",
        "PySPark does not come native to Google Colab\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SLsICX4sQ-rO",
        "outputId": "97d6e169-022a-420e-ad4a-efa5fa998012"
      },
      "source": [
        "import os\r\n",
        "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\r\n",
        "# For example:\r\n",
        "# spark_version = 'spark-3.0.1'\r\n",
        "spark_version = 'spark-3.0.1'\r\n",
        "os.environ['SPARK_VERSION']=spark_version\r\n",
        "\r\n",
        "# Install Spark and Java\r\n",
        "!apt-get update\r\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\r\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
        "!pip install -q findspark\r\n",
        "\r\n",
        "# Set Environment Variables\r\n",
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\r\n",
        "\r\n",
        "# Start a SparkSession\r\n",
        "import findspark\r\n",
        "findspark.init()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [1 InRelease 14.2 kB/88.7\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [1 InRelease 63.4 kB/88.7 kB 71%] [Waiting for headers\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [4 InRelease 14.2 kB/15.9 k\r                                                                               \rIgn:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [4 InRelease 14.2 kB/15.9 k\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [4 InRelease 14.2 kB/15.9 k\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [4 InRelease 14.2 kB/15.9 k\r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,727 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [884 kB]\n",
            "Fetched 2,878 kB in 3s (823 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLxVPko_REU3"
      },
      "source": [
        "Use Spark to write directly to the Postgres database (my_data_class_db).   \r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HnMoihH4UHch",
        "outputId": "4ee585ad-b27b-471b-c2a2-7fb4f132156f"
      },
      "source": [
        "# Download Postgres Driver (1-time operation)\r\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-07 03:33:30--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1002883 (979K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.16.jar.1’\n",
            "\n",
            "postgresql-42.2.16. 100%[===================>] 979.38K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-02-07 03:33:30 (6.63 MB/s) - ‘postgresql-42.2.16.jar.1’ saved [1002883/1002883]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12it0bHMVhHG"
      },
      "source": [
        "# Start a SparkSession\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()\r\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTVbKGztW4o6"
      },
      "source": [
        "**Extract**: Get Raw Data Stored in AWS S3 into a PySpark Dataframe\r\n",
        "\r\n",
        "---\r\n",
        "1. **Import SparkFiles from PySpark** into this notebook. This allows Spark to add a file to the Spark project. \r\n",
        "2. **Read file** in with the read method and combined with the csv() method, which pulls in the CSV stored in SparkFiles and infers the schema. SparkFiles.get() will have Spark retrieve the specified file.\r\n",
        "3.  An action is called to **show the first 10 runs** and confirm our data extraction.\r\n",
        "4. Complete process to load both **user_data.csv** and \t**user_payment.csv**.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lsAvRRr_aCaM",
        "outputId": "73a843ba-a606-41e4-a0ee-74d512a32416"
      },
      "source": [
        "# user_data.csv: Read in data from S3 Buckets\r\n",
        "from pyspark import SparkFiles\r\n",
        "url =\"https://hankmikebootcamp.s3.amazonaws.com/user_data.csv\"\r\n",
        "spark.sparkContext.addFile(url)\r\n",
        "user_data_df = spark.read.csv(SparkFiles.get(\"user_data.csv\"), sep=\",\", header=True, inferSchema=True)\r\n",
        "\r\n",
        "# Show DataFrame\r\n",
        "user_data_df.show(10)\r\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------+---------+-----------+--------------------+--------------------+-----------+\n",
            "| id|first_name|last_name|active_user|      street_address|               state|   username|\n",
            "+---+----------+---------+-----------+--------------------+--------------------+-----------+\n",
            "|  1|    Cletus|  Lithcow|      false| 78309 Riverside Way|            Virginia|  ibearham0|\n",
            "|  2|       Caz|   Felgat|      false| 83 Hazelcrest Place|             Alabama|   wwaller1|\n",
            "|  3|     Kerri|  Crowson|      false|      112 Eliot Pass|      North Carolina|  ichesnut2|\n",
            "|  4|   Freddie|    Caghy|      false|     15 Merchant Way|            New York|    tsnarr3|\n",
            "|  5|   Sadella|    Deuss|      false|    079 Acker Avenue|           Tennessee|  fwherrit4|\n",
            "|  6|    Fraser|  Korneev|       true|  76084 Novick Court|           Minnesota| fstappard5|\n",
            "|  7|    Demott|   Rapson|       true|    86320 Dahle Park|District of Columbia| lhambling6|\n",
            "|  8|    Robert|    Poile|      false|1540 Manitowish Hill|             Georgia|     drude7|\n",
            "|  9|    Nollie|     null|       true|       4 Katie Court|                Ohio|  bspawton8|\n",
            "| 10|   Merilyn| Frascone|      false|     387 Duke Street|                Ohio|rmackeller9|\n",
            "+---+----------+---------+-----------+--------------------+--------------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "za5nLI_kagFC",
        "outputId": "46895f85-c3ba-4324-8d59-5332a1c1c323"
      },
      "source": [
        "# user_data.csv: Read in data from S3 Buckets\r\n",
        "from pyspark import SparkFiles\r\n",
        "url =\"https://hankmikebootcamp.s3.amazonaws.com/user_payment.csv\"\r\n",
        "spark.sparkContext.addFile(url)\r\n",
        "user_payment_df = spark.read.csv(SparkFiles.get(\"user_payment.csv\"), sep=\",\", header=True, inferSchema=True)\r\n",
        "\r\n",
        "# Show DataFrame\r\n",
        "user_payment_df.show(10)\r\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----------+--------------------+\n",
            "|billing_id|   username|        cc_encrypted|\n",
            "+----------+-----------+--------------------+\n",
            "|         1|  ibearham0|a799fcafe47d7fb19...|\n",
            "|         2|   wwaller1|a799fcafe47d7fb19...|\n",
            "|         3|  ichesnut2|a799fcafe47d7fb19...|\n",
            "|         4|    tsnarr3|a799fcafe47d7fb19...|\n",
            "|         5|  fwherrit4|a799fcafe47d7fb19...|\n",
            "|         6| fstappard5|a799fcafe47d7fb19...|\n",
            "|         7| lhambling6|a799fcafe47d7fb19...|\n",
            "|         8|     drude7|a799fcafe47d7fb19...|\n",
            "|         9|  bspawton8|a799fcafe47d7fb19...|\n",
            "|        10|rmackeller9|a799fcafe47d7fb19...|\n",
            "+----------+-----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhE-vu7Ra8Y0"
      },
      "source": [
        "**Transform**: Manipulate Data in **user_data_df ** and **user_payment_df**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "1. Join the two tables\r\n",
        "2. Drop rows with null or \"not a number\" (NaN) values\r\n",
        "3. Filter for active users\r\n",
        "4. Select columns to create 3 DFs that match the AWS RDS database.  \r\n",
        "*   Create a DataFrame to match the active_user table\r\n",
        "*   Create a DataFrame to match the billing_info table\r\n",
        "*   Create a DataFrame to match the payment_info table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fWJV1Smna9sp",
        "outputId": "868b83cf-cbce-4b28-9fe1-d1919340938d"
      },
      "source": [
        "# 1. Join the two Dataframes\r\n",
        "joined_df = user_data_df.join(user_payment_df, on=\"username\", how=\"inner\")\r\n",
        "joined_df.show(10)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
            "|   username| id|first_name|last_name|active_user|      street_address|               state|billing_id|        cc_encrypted|\n",
            "+-----------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
            "|  ibearham0|  1|    Cletus|  Lithcow|      false| 78309 Riverside Way|            Virginia|         1|a799fcafe47d7fb19...|\n",
            "|   wwaller1|  2|       Caz|   Felgat|      false| 83 Hazelcrest Place|             Alabama|         2|a799fcafe47d7fb19...|\n",
            "|  ichesnut2|  3|     Kerri|  Crowson|      false|      112 Eliot Pass|      North Carolina|         3|a799fcafe47d7fb19...|\n",
            "|    tsnarr3|  4|   Freddie|    Caghy|      false|     15 Merchant Way|            New York|         4|a799fcafe47d7fb19...|\n",
            "|  fwherrit4|  5|   Sadella|    Deuss|      false|    079 Acker Avenue|           Tennessee|         5|a799fcafe47d7fb19...|\n",
            "| fstappard5|  6|    Fraser|  Korneev|       true|  76084 Novick Court|           Minnesota|         6|a799fcafe47d7fb19...|\n",
            "| lhambling6|  7|    Demott|   Rapson|       true|    86320 Dahle Park|District of Columbia|         7|a799fcafe47d7fb19...|\n",
            "|     drude7|  8|    Robert|    Poile|      false|1540 Manitowish Hill|             Georgia|         8|a799fcafe47d7fb19...|\n",
            "|  bspawton8|  9|    Nollie|     null|       true|       4 Katie Court|                Ohio|         9|a799fcafe47d7fb19...|\n",
            "|rmackeller9| 10|   Merilyn| Frascone|      false|     387 Duke Street|                Ohio|        10|a799fcafe47d7fb19...|\n",
            "+-----------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OvhL_nhihutx",
        "outputId": "6f3eae62-6c30-4392-da56-6ac81c71e044"
      },
      "source": [
        "# 2. Drop null values\r\n",
        "dropna_df = joined_df.dropna()\r\n",
        "dropna_df.show(10)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
            "|   username| id|first_name|last_name|active_user|      street_address|               state|billing_id|        cc_encrypted|\n",
            "+-----------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
            "|  ibearham0|  1|    Cletus|  Lithcow|      false| 78309 Riverside Way|            Virginia|         1|a799fcafe47d7fb19...|\n",
            "|   wwaller1|  2|       Caz|   Felgat|      false| 83 Hazelcrest Place|             Alabama|         2|a799fcafe47d7fb19...|\n",
            "|  ichesnut2|  3|     Kerri|  Crowson|      false|      112 Eliot Pass|      North Carolina|         3|a799fcafe47d7fb19...|\n",
            "|    tsnarr3|  4|   Freddie|    Caghy|      false|     15 Merchant Way|            New York|         4|a799fcafe47d7fb19...|\n",
            "|  fwherrit4|  5|   Sadella|    Deuss|      false|    079 Acker Avenue|           Tennessee|         5|a799fcafe47d7fb19...|\n",
            "| fstappard5|  6|    Fraser|  Korneev|       true|  76084 Novick Court|           Minnesota|         6|a799fcafe47d7fb19...|\n",
            "| lhambling6|  7|    Demott|   Rapson|       true|    86320 Dahle Park|District of Columbia|         7|a799fcafe47d7fb19...|\n",
            "|     drude7|  8|    Robert|    Poile|      false|1540 Manitowish Hill|             Georgia|         8|a799fcafe47d7fb19...|\n",
            "|rmackeller9| 10|   Merilyn| Frascone|      false|     387 Duke Street|                Ohio|        10|a799fcafe47d7fb19...|\n",
            "|cdennerleya| 11|    Rickie| Tredwell|      false|  04 Monterey Center|            Missouri|        11|a799fcafe47d7fb19...|\n",
            "+-----------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qeIgpzGvjMhW",
        "outputId": "f129e145-48fd-44a3-e0b4-e82dc8a0eb6d"
      },
      "source": [
        "# 3. Filter for Active Users\r\n",
        "\r\n",
        "# Load in a sql function to use columns\r\n",
        "from pyspark.sql.functions import col\r\n",
        "\r\n",
        "# Filter for only columns withy active users\r\n",
        "cleaned_df = dropna_df.filter(col(\"active_user\")==True)\r\n",
        "cleaned_df.show(10)\r\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
            "|    username| id|first_name|last_name|active_user|      street_address|               state|billing_id|        cc_encrypted|\n",
            "+------------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
            "|  fstappard5|  6|    Fraser|  Korneev|       true|  76084 Novick Court|           Minnesota|         6|a799fcafe47d7fb19...|\n",
            "|  lhambling6|  7|    Demott|   Rapson|       true|    86320 Dahle Park|District of Columbia|         7|a799fcafe47d7fb19...|\n",
            "|   wheinerte| 15|   Sadella|    Jaram|       true|7528 Waxwing Terrace|         Connecticut|        15|a799fcafe47d7fb19...|\n",
            "|droughsedgeg| 17|    Hewitt|  Trammel|       true|    2455 Corry Alley|      North Carolina|        17|a799fcafe47d7fb19...|\n",
            "|   ydudeniei| 19|       Ted|  Knowlys|       true|      31 South Drive|                Ohio|        19|a799fcafe47d7fb19...|\n",
            "|    fmyttonm| 23|  Annmarie|   Lafond|       true|     35 Oriole Place|             Georgia|        23|a799fcafe47d7fb19...|\n",
            "|  bfletcherr| 28|      Toma|   Sokell|       true|39641 Eggendart Hill|            Maryland|        28|a799fcafe47d7fb19...|\n",
            "|    gturleyt| 30|       Ram|  Lefever|       true|   9969 Laurel Alley|               Texas|        30|a799fcafe47d7fb19...|\n",
            "|   calyukinu| 31|    Raddie|  Heindle|       true|   811 Talmadge Road|                Ohio|        31|a799fcafe47d7fb19...|\n",
            "|ckleinlererw| 33|    Wallie|     Caws|       true|   9999 Kenwood Pass|              Oregon|        33|a799fcafe47d7fb19...|\n",
            "+------------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-2UHpNKAkYO2",
        "outputId": "1b13267e-b0f5-456e-aefb-6ac3a21d7756"
      },
      "source": [
        "# 4.a. Create a DataFrame to match the active_user table\r\n",
        "clean_user_df = cleaned_df.select([\"id\", \"first_name\", \"last_name\", \"username\"])\r\n",
        "clean_user_df.show(10)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------+---------+------------+\n",
            "| id|first_name|last_name|    username|\n",
            "+---+----------+---------+------------+\n",
            "|  6|    Fraser|  Korneev|  fstappard5|\n",
            "|  7|    Demott|   Rapson|  lhambling6|\n",
            "| 15|   Sadella|    Jaram|   wheinerte|\n",
            "| 17|    Hewitt|  Trammel|droughsedgeg|\n",
            "| 19|       Ted|  Knowlys|   ydudeniei|\n",
            "| 23|  Annmarie|   Lafond|    fmyttonm|\n",
            "| 28|      Toma|   Sokell|  bfletcherr|\n",
            "| 30|       Ram|  Lefever|    gturleyt|\n",
            "| 31|    Raddie|  Heindle|   calyukinu|\n",
            "| 33|    Wallie|     Caws|ckleinlererw|\n",
            "+---+----------+---------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IsCDzZGulMJ6",
        "outputId": "8add83ac-367d-4ad2-c631-c6c7790e120f"
      },
      "source": [
        "# 4.b.Create a DataFrame to match the billing_info table\r\n",
        "clean_billing_df = cleaned_df.select([\"billing_id\", \"street_address\", \"state\", \"username\"])\r\n",
        "clean_billing_df.show(10)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+--------------------+------------+\n",
            "|billing_id|      street_address|               state|    username|\n",
            "+----------+--------------------+--------------------+------------+\n",
            "|         6|  76084 Novick Court|           Minnesota|  fstappard5|\n",
            "|         7|    86320 Dahle Park|District of Columbia|  lhambling6|\n",
            "|        15|7528 Waxwing Terrace|         Connecticut|   wheinerte|\n",
            "|        17|    2455 Corry Alley|      North Carolina|droughsedgeg|\n",
            "|        19|      31 South Drive|                Ohio|   ydudeniei|\n",
            "|        23|     35 Oriole Place|             Georgia|    fmyttonm|\n",
            "|        28|39641 Eggendart Hill|            Maryland|  bfletcherr|\n",
            "|        30|   9969 Laurel Alley|               Texas|    gturleyt|\n",
            "|        31|   811 Talmadge Road|                Ohio|   calyukinu|\n",
            "|        33|   9999 Kenwood Pass|              Oregon|ckleinlererw|\n",
            "+----------+--------------------+--------------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ifOvboUTlvxM",
        "outputId": "dc8fccc2-b750-49da-8768-7a1533851257"
      },
      "source": [
        "# 4.c. Create a DataFrame to match the payment_info table\r\n",
        "clean_payment_df = cleaned_df.select([\"billing_id\", \"cc_encrypted\"])\r\n",
        "clean_payment_df.show(10)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+\n",
            "|billing_id|        cc_encrypted|\n",
            "+----------+--------------------+\n",
            "|         6|a799fcafe47d7fb19...|\n",
            "|         7|a799fcafe47d7fb19...|\n",
            "|        15|a799fcafe47d7fb19...|\n",
            "|        17|a799fcafe47d7fb19...|\n",
            "|        19|a799fcafe47d7fb19...|\n",
            "|        23|a799fcafe47d7fb19...|\n",
            "|        28|a799fcafe47d7fb19...|\n",
            "|        30|a799fcafe47d7fb19...|\n",
            "|        31|a799fcafe47d7fb19...|\n",
            "|        33|a799fcafe47d7fb19...|\n",
            "+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cs-J2EamMrf"
      },
      "source": [
        "**Load**: Move transformed raw data **into the AWS Relational Database**.  PySpark will connect to the RDS database to load the DataFrames into the table.\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "1. Configure AWS RDS settings to allow the connection\r\n",
        "2. Write the cleaned DataFrames directly to the AWS database\r\n",
        "3. Verify work by running querries in pgAdmin on the AWS database to confirm a successful load of data to AWS RDS.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsex6v-mn89p"
      },
      "source": [
        "# 1. Configure AWS RDS settings to allow the connection\r\n",
        "mode = \"append\"\r\n",
        "jdbc_url=\"jdbc:postgresql://mypostgresdb.crj7eeu0fkul.us-east-1.rds.amazonaws.com:5432/my_data_class_db\"\r\n",
        "config = {\"user\":\"root\",\r\n",
        "          \"password\": \"postgres\",\r\n",
        "          \"driver\":\"org.postgresql.Driver\"}\r\n",
        "\r\n",
        "\r\n",
        "#   my_data_class_db    mypostgresdb"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08nKyDHQoj2T"
      },
      "source": [
        "# 2.Write the cleaned DataFrames directly to the AWS database\r\n",
        "\r\n",
        "# Write DataFrame to active_user table in RDS\r\n",
        "clean_user_df.write.jdbc(url=jdbc_url, table='active_user', mode=mode, properties=config)\r\n",
        "\r\n",
        "# Write dataframe to billing_info table in RDS\r\n",
        "clean_billing_df.write.jdbc(url=jdbc_url, table='billing_info', mode=mode, properties=config)\r\n",
        "\r\n",
        "# Write dataframe to payment_info table in RDS\r\n",
        "clean_payment_df.write.jdbc(url=jdbc_url, table='payment_info', mode=mode, properties=config)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypoejsjEosBn"
      },
      "source": [
        "3. Verify work by running querries **in pgAdmin** on the AWS database to confirm a successful load of data to AWS RDS."
      ]
    }
  ]
}